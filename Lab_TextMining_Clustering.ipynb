{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naomi Nayman, Jack Herberger, Tyler Baxter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMHN8yHbG2Lg"
   },
   "source": [
    "###Assignment: Analyzing Book Genres and Themes through Clustering\n",
    "\n",
    "Full Credit: 20 points\n",
    "\n",
    "You can either work independently or in a group of three. Should you choose to work in a group, you MUST mention the group member names. You can only work in groups of three ie each group should have only 3 members.\n",
    "\n",
    "#### **Objective:**\n",
    "The goal of this assignment is to utilize clustering techniques on a dataset of book descriptions to achieve the following:\n",
    "\n",
    "1. **Discover Common Themes:**\n",
    "   - Identify prevalent themes within clusters of books.\n",
    "2. **Assign Genres to Authors:**\n",
    "   - Group books into clusters and determine the genre for each author based on their cluster membership.\n",
    "\n",
    "#### **Functionality Requirements:**\n",
    "Your program should:\n",
    "- Display the genre associated with a given author. (**10 points**)\n",
    "- Identify and display the common themes of a specific authorâ€™s works. (**5 points**)\n",
    "- Visualize using a Word cloud (**5 points**)\n",
    "---\n",
    "\n",
    "#### **Dataset:**\n",
    "Use the `cleaned_books.csv` dataset that can be downloaded that includes book descriptions and author details.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Tasks:**\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Load the dataset into a Pandas DataFrame.\n",
    "   - Clean the book descriptions by removing stop words and performing additional necessary preprocessing steps.\n",
    "\n",
    "2. **Clustering:**\n",
    "   - Generate TF-IDF vectors from the book descriptions using `TfidfVectorizer`.\n",
    "   - Identify the optimal number of clusters (`k`) using a suitable method (e.g., the Elbow Method).\n",
    "   - Choose a clustering algorithm (e.g., K-Means, DBSCAN) and justify your choice.\n",
    "   - Perform clustering to group books into clusters and assign a cluster label to each book in the DataFrame.\n",
    "\n",
    "3. **Genre Assignment:**\n",
    "   - Group books by author and cluster labels.\n",
    "   - Assign the most frequent cluster associated with each author as their genre.\n",
    "   - Create a data structure (e.g., a Pandas Series) to store the mapping of authors to their assigned genres.\n",
    "\n",
    "4. **Theme Identification:**\n",
    "   - Develop a function to extract the main themes of a cluster using the top `N` terms from the TF-IDF matrix.\n",
    "   - Apply this function to all clusters to identify and interpret the predominant themes.\n",
    "   - Visualize using a Word cloud\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "```\n",
    "Use CAses: (the below use case is with K Means clustering. You can get different cluster number and different words under themes depending on your implementation. Moreover, you can apply DBScan as well.)\n",
    "\n",
    "Enter an author name: Stephen King\n",
    "The genre of Stephen King is: Cluster 1\n",
    "\n",
    "Common themes for Stephen King: 'concis' 'hemingway' 'autobiograph' ... 'power' 'one' 'world'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhKc0V_oIH50"
   },
   "source": [
    "Use the following helper code to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGRsJDuRIKw8"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Download the zip file from the raw URL\n",
    "url = \"https://github.com/sumonacalpoly/Datasets/raw/main/cleaned_books.csv.zip\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Raise an exception for bad responses\n",
    "\n",
    "# Extract the CSV data from the zip file\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as zf:\n",
    "    with zf.open('cleaned_books.csv') as f:\n",
    "        cleaned_books = pd.read_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99eXEGH4IMCa"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = cleaned_books.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NK89LwwQC-Hu"
   },
   "outputs": [],
   "source": [
    "%pip install wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQKsi6QOBF_f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
